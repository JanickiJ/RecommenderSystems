{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 4 - rekomendacje dla portali informacyjnych\n",
    "\n",
    "## Przygotowanie\n",
    "\n",
    " * pobierz i wypakuj dataset: https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip\n",
    "   * więcej możesz poczytać tutaj: https://learn.microsoft.com/en-us/azure/open-datasets/dataset-microsoft-news\n",
    " * [opcjonalnie] Utwórz wirtualne środowisko\n",
    " `python3 -m venv ./recsyslab4`\n",
    " * zainstaluj potrzebne biblioteki:\n",
    " `pip install nltk sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 1. - przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kuba/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /home/kuba/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importujemy wszystkie potrzebne pakiety\n",
    "\n",
    "import codecs\n",
    "import string\n",
    "from collections import defaultdict  # mozesz uzyc zamiast zwyklego slownika, rozwaz wplyw na czas obliczen\n",
    "import math\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# mozesz uzyc do obliczania najbardziej podobnych tekstow zamiast liczenia \"na piechote\"\n",
    "# ale pamietaj o dostosowaniu formatu danych\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definiujemy potrzebne zmienne\n",
    "\n",
    "PATH = './MINDsmall_train'\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51282\n"
     ]
    }
   ],
   "source": [
    "# wczytujemy metadane artykułów\n",
    "\n",
    "def parse_news_entry(entry):\n",
    "    news_id, category, subcategory, title, abstract = entry.split('\\t')[:5]\n",
    "    return {\n",
    "        'news_id': news_id,\n",
    "        'category': category,\n",
    "        'subcategory': subcategory,\n",
    "        'title': title,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "\n",
    "\n",
    "def get_news_metadata():\n",
    "    with codecs.open(f'{PATH}/news.tsv', 'r', 'UTF-8') as f:\n",
    "        raw = [x for x in f.read().split('\\n') if x]\n",
    "        parsed_entries = [parse_news_entry(entry) for entry in raw]\n",
    "        return {x['news_id']: x for x in parsed_entries}\n",
    "\n",
    "\n",
    "news = get_news_metadata()\n",
    "news_ids = sorted(list(news.keys()))\n",
    "news_indices = {x[1]: x[0] for x in enumerate(news_ids)}\n",
    "print(len(news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 2. - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizujemy teksty na potrzeby dalszego przetwarzania\n",
    "\n",
    "def preprocess_text(text_in):\n",
    "    text = text_in\n",
    "    # zamieniamy wszystkie ciagi bialych znakow na pojedyncze spacje\n",
    "    text = ' '.join(text.split())\n",
    "    # usuwamy znaki interpunkcyjne\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # usuwamy wszystkie liczby\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    # podmieniamy wszystkie wielkie litery\n",
    "    text = text.lower()\n",
    "    # dzielimy na tokeny\n",
    "    tokens = text.split()\n",
    "    # usuwamy stopwords\n",
    "    tokens_without_stopwords = [word for word in tokens if word not in STOPWORDS]\n",
    "    return tokens_without_stopwords\n",
    "\n",
    "\n",
    "def stem_texts(corpus):\n",
    "    stemmer = LancasterStemmer()  # przetestuj rozne stemmery\n",
    "\n",
    "    return [[stemmer.stem(word) for word in preprocess_text(text)] for text in corpus]\n",
    "\n",
    "\n",
    "texts = [news[news_id]['abstract'] for news_id in news_ids]\n",
    "stemmed_texts = stem_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I think we have a really good team, and a team that can really do some special, good things because that group is very close in there.\" - Brian Schmetzer\n",
      "\n",
      "think real good team team real spec good thing group clos bri schmetzer\n"
     ]
    }
   ],
   "source": [
    "# porownajmy teksty przed i po przetworzeniu\n",
    "\n",
    "print(texts[2] + '\\n')\n",
    "print(' '.join(stemmed_texts[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37021\n"
     ]
    }
   ],
   "source": [
    "# tworzymy liste wszystkich slow w korpusie\n",
    "\n",
    "def get_all_words_sorted(corpus):\n",
    "    # generujemy posortowana alfabetycznie liste wszystkich slow (tokenow)\n",
    "    corpus = [token for text in corpus for token in text]\n",
    "    return sorted(list(set(corpus)))\n",
    "\n",
    "\n",
    "wordlist = get_all_words_sorted(stemmed_texts)\n",
    "word_indices = {x[1]: x[0] for x in enumerate(wordlist)}\n",
    "print(len(wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczamy liczbe tekstow, w ktorych wystapilo kazde ze slow\n",
    "# pamietaj, ze jesli slowo wystapilo w danym tekscie wielokrotnie, to liczymy je tylko raz\n",
    "\n",
    "def get_document_frequencies(corpus, wordlist):\n",
    "    counter = {word:0 for word in wordlist}\n",
    "    for text in corpus:\n",
    "        for word in set(text):\n",
    "            counter[word] = +1\n",
    "\n",
    "    # return {word -> count}\n",
    "    return counter\n",
    "\n",
    "\n",
    "document_frequency = get_document_frequencies(stemmed_texts, wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczamy liczbe wystapien kazdego slowa w kazdym tekscie\n",
    "\n",
    "def get_term_frequencies(corpus, news_indices):\n",
    "    # return {news_id -> {word -> count}}\n",
    "    counter = {new_id:{} for new_id in news_indices}\n",
    "    for new_idx, corpus_idx in news_indices.items():\n",
    "        new = corpus[corpus_idx]\n",
    "        for word in new:\n",
    "            if not word in counter[new_idx]:\n",
    "                counter[new_idx][word] = 0\n",
    "            counter[new_idx][word] += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "term_frequency = get_term_frequencies(stemmed_texts, news_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'think': 1,\n 'real': 2,\n 'good': 2,\n 'team': 2,\n 'spec': 1,\n 'thing': 1,\n 'group': 1,\n 'clos': 1,\n 'bri': 1,\n 'schmetzer': 1}"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sprawdzmy wyniki\n",
    "\n",
    "term_frequency[news_ids[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczamy metryke tf_idf\n",
    "\n",
    "def calculate_tf_idf(term_frequency, document_frequency, corpus_size):\n",
    "    tf_idfs = {}\n",
    "    n = corpus_size\n",
    "\n",
    "    for new_id in news_indices.keys():\n",
    "        tf_idfs[new_id] = {}\n",
    "        for word in term_frequency[new_id].keys():\n",
    "            df = document_frequency[word]\n",
    "            tf = term_frequency[new_id][word]\n",
    "            tf_idfs[new_id][word] = tf * math.log(n / df)\n",
    "\n",
    "    # return {news_id -> {word -> tf_idf}}\n",
    "    return tf_idfs\n",
    "\n",
    "\n",
    "tf_idf = calculate_tf_idf(term_frequency, document_frequency, len(news_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'think': 10.845095092394073,\n 'real': 21.690190184788147,\n 'good': 21.690190184788147,\n 'team': 21.690190184788147,\n 'spec': 10.845095092394073,\n 'thing': 10.845095092394073,\n 'group': 10.845095092394073,\n 'clos': 10.845095092394073,\n 'bri': 10.845095092394073,\n 'schmetzer': 10.845095092394073}"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sprawdzmy wyniki\n",
    "\n",
    "tf_idf[news_ids[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 3. - Podobieństwo tekstów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "-1"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# obliczmy odleglosc miedzy dwoma artykulami\n",
    "# przetestuj rozne metryki odleglosci i wybierz najlepsza\n",
    "\n",
    "def euclidean_metrics(new_1, new_2):\n",
    "    words = list(new_1.keys())+list(new_2.keys())\n",
    "    euclidean_sum = 0\n",
    "    for word in words:\n",
    "        x = new_1.get(word,0)\n",
    "        y = new_2.get(word,0)\n",
    "        euclidean_sum += (y - x) ** 2\n",
    "    return math.sqrt(euclidean_sum)\n",
    "\n",
    "\n",
    "def cosine_metrics(new_1, new_2):\n",
    "    words = list(new_1.keys())+list(new_2.keys())\n",
    "    x, y = [], []\n",
    "    for word in words:\n",
    "        x.append(new_1.get(word,0))\n",
    "        y.append(new_2.get(word,0))\n",
    "    res = dot(x, y) / (norm(x) * norm(y))\n",
    "    if not res:\n",
    "        return -1\n",
    "    return res\n",
    "\n",
    "\n",
    "def calculate_distance(tf_idf, id1, id2, metrics=\"cosine\"):\n",
    "    new_1, new_2 = tf_idf[id1], tf_idf[id2]\n",
    "    if metrics == \"cosine\":\n",
    "        return cosine_metrics(new_1, new_2)\n",
    "    if metrics == \"euclidean\":\n",
    "        return euclidean_metrics(new_1, new_2)\n",
    "\n",
    "\n",
    "calculate_distance(tf_idf, news_ids[0], news_ids[1], \"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63128/1724528135.py:20: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  res = dot(x, y) / (norm(x) * norm(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: N1000, text: \"I think we have a really good team, and a team that can really do some special, good things because that group is very close in there.\" - Brian Schmetzer\n",
      "\n",
      "5 most similar:\n",
      "\n",
      "id: N10109, text: \"There are an awful lot of really good journalists at Fox News Channel. It's just that they're vastly outnumbered by the opinion makers,\" Carl Cameron says. Julie Roginsky and Conor Powell also speak about Shep Smith's importance to the network and what his exit signifies.\n",
      "\n",
      "id: N100, text: WASHINGTON   Somewhere over the eastern skies, the Astros assembled again as one. On an airplane to the nation's capital, the team conferred about their conundrum. A players-only meeting after Wednesday's wretched loss in Game 2 was run by two veterans who calmed any concern that cropped up. Justin Verlander and Jose Altuve emphasized encouragement. They reminded their teammates of a remarkable regular season and the prestige within the...\n",
      "\n",
      "id: N10023, text: SCOTTSDALE, Ariz.   The Astros can find comfort atop their 2020 starting rotation. Justin Verlander and Zack Greinke are future Hall of Famers defying age to anchor a staff that is primed for seismic changes. Behind those two certainties are almost no absolutes, a reality the Astros must confront this winter. Though his team urgently require a catcher and middle relief, general manager Jeff Luhnow seems concentrated on another truth. \"We're...\n",
      "\n",
      "id: N10009, text: There is a point where two baseball teams must give up on proving which is the better of them and settle for which of them can win a baseball game.\n",
      "\n",
      "id: N10058, text: These types of galaxies were thought to be folklore due to a lack of evidence. A University of Arizona-led team accidentally stumbled upon the signal that was so far away, it took 12.5 billion years to reach us. Researchers says the \"cosmic Yeti\" could lead to the discovery of a whole group of galaxies.\n"
     ]
    }
   ],
   "source": [
    "# wyznaczmy k najpodobniejszych tekstow do danego\n",
    "# pamietaj o odpowiedniej kolejnosci sortowania w zaleznosci od wykorzystanej metryki\n",
    "# pamietaj, zeby wsrod podobnych tekstow nie bylo danego\n",
    "\n",
    "def get_k_most_similar_news(tf_idf, n_id, k, metrics):\n",
    "    distances = []\n",
    "    for new_id in filter(lambda id: id != n_id, news_indices.keys()):\n",
    "        distance = calculate_distance(tf_idf, n_id, new_id, metrics)\n",
    "        distances.append([new_id, distance])\n",
    "    if metrics == \"cosine\":\n",
    "        distances.sort(key=lambda x: abs(x[1] - 1))\n",
    "    if metrics == \"euclidean\":\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "    return list(map(lambda x: x[0], distances[:k]))\n",
    "\n",
    "\n",
    "def print_k_most_similar_news(tf_idf, n_id, k, corpus, news_indices, metrics):\n",
    "    similar = get_k_most_similar_news(tf_idf, n_id, k, metrics)\n",
    "    print(f'id: {n_id}, text: {corpus[news_indices[n_id]]}')\n",
    "    print(f'\\n{k} most similar:')\n",
    "    for s_id in similar:\n",
    "        print(f'\\nid: {s_id}, text: {corpus[news_indices[s_id]]}')\n",
    "\n",
    "\n",
    "print_k_most_similar_news(tf_idf, news_ids[2], 5, texts, news_indices, \"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: N1000, text: \"I think we have a really good team, and a team that can really do some special, good things because that group is very close in there.\" - Brian Schmetzer\n",
      "\n",
      "5 most similar:\n",
      "\n",
      "id: N12893, text: Might be a good thing?\n",
      "\n",
      "id: N34755, text: It really hurts to watch this team.\n",
      "\n",
      "id: N47189, text: Not good!\n",
      "\n",
      "id: N11497, text: Experts get real about these specs.\n",
      "\n",
      "id: N16082, text: she had a good night\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_k_most_similar_news(tf_idf, news_ids[2], 5, texts, news_indices, \"euclidean\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
